{"componentChunkName":"component---src-templates-post-js","path":"/hyperloglogs/","result":{"data":{"markdownRemark":{"html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6a71631a71c02df2b051e1d802685d7f/b1218/hash.webp\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.57142857142857%; position: relative; bottom: 0; left: 0; background-image: url('data:image/webp;base64,UklGRjwAAABXRUJQVlA4IDAAAADQAgCdASoUAAsAPtFUo0uoJKMhsAgBABoJZwAAhAwAAP7u2QkqThzRe1+O6d6AAAA='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"hash\"\n        title=\"\"\n        src=\"/static/6a71631a71c02df2b051e1d802685d7f/fa73e/hash.webp\"\n        srcset=\"/static/6a71631a71c02df2b051e1d802685d7f/9cece/hash.webp 175w,\n/static/6a71631a71c02df2b051e1d802685d7f/adf5f/hash.webp 350w,\n/static/6a71631a71c02df2b051e1d802685d7f/fa73e/hash.webp 700w,\n/static/6a71631a71c02df2b051e1d802685d7f/7e3cb/hash.webp 1050w,\n/static/6a71631a71c02df2b051e1d802685d7f/aa619/hash.webp 1400w,\n/static/6a71631a71c02df2b051e1d802685d7f/b1218/hash.webp 2000w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>One of the coolest data structures in back end systems is the Bloom filter's\ncousin, the <strong>HyperLogLog</strong>.</p>\n<p>Here's the problem: you need to run a distinct count over millions of rows, but that takes time,\nCPU, and memory.</p>\n<p>That's where HyperLogLog comes in.</p>\n<p>It's a probabilistic data structure that estimates the number of unique elements in a stream fast and with almost no memory.</p>\n<p>Here's how it works:</p>\n<ul>\n<li>Every input is passed through a good hash function like murmur hash or XX hash, which spreads values uniformly across a massive binary space.</li>\n<li>Next, HyperLogLog uses the first P bits of the hash to pick a bucket, then counts the number of leading zeros in the remaining bits.</li>\n<li>The idea is simple - the more distinct values you've seen, the more likely you are to get a hash with a lot of leading zeros. So if you see something like this, that's rare and strong evidence that you've seen a lot of unique items.</li>\n<li>Each bucket tracks the maximum number of leading zeros it has seen, and once all the elements are processed, HyperLogLog combines the Max zero run values using a special harmonic mean formula to estimate the total cardinality.</li>\n<li>The result is a high confidence estimate for the number of unique values that you've seen using only kilobytes of memory.</li>\n</ul>\n<p>HyperLogLog shows up everywhere - in Redis, Postgres, BigQuery, even in monitoring tools like Datadog.</p>\n<p>So if your app needs an accurate estimate for the number of records even at billions of rows, give HyperLogLog a try - it's only an <code class=\"language-text\">npm</code> or <code class=\"language-text\">pip</code> install away.</p>\n<p><a href=\"https://www.linkedin.com/in/john-pratt787\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Follow</a> for more dev tips!</p>","frontmatter":{"title":"The \"HyperLogLog\"","draft":false,"description":"a","date":"2025-06-06","slug":"/hyperloglogs","tags":["data-structures","algorithms","databases","data"]}}},"pageContext":{"slug":"/hyperloglogs"}},"staticQueryHashes":["1466254954","2744132034","2759103021","3539500889"],"slicesMap":{}}